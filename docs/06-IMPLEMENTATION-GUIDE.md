# MeetingMod - Implementation Guide for Codex

## 1. êµ¬í˜„ ìš°ì„ ìˆœìœ„ (í•´ì»¤í†¤ 7ì‹œê°„ ê¸°ì¤€)

### Phase 1: ê¸°ë³¸ ì…‹ì—… (1ì‹œê°„)
```
Priority: CRITICAL
ëª©í‘œ: í”„ë¡œì íŠ¸ êµ¬ì¡° & ê¸°ë³¸ UI ì…‹ì—…
```

1. **í”„ë¡œì íŠ¸ ì´ˆê¸°í™”**
   - Next.js 14 (App Router) í”„ë¡œì íŠ¸ ìƒì„±
   - Python 3.12 + OpenAI Agents SDK ë°±ì—”ë“œ í”„ë¡œì íŠ¸ ìƒì„±
   - í•„ìˆ˜ ì˜ì¡´ì„± ì„¤ì¹˜

2. **ê¸°ë³¸ UI ì»´í¬ë„ŒíŠ¸**
   - shadcn/ui ì„¤ì¹˜ ë° êµ¬ì„±
   - ê¸°ë³¸ ë ˆì´ì•„ì›ƒ ì»´í¬ë„ŒíŠ¸
   - íšŒì˜ ì¤€ë¹„ í™”ë©´ ê¸°ë³¸ êµ¬ì¡°

### Phase 2: í•µì‹¬ ê¸°ëŠ¥ - ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ (3ì‹œê°„)
```
Priority: CRITICAL
ëª©í‘œ: STT â†’ Agent â†’ ê°œì… íŒŒì´í”„ë¼ì¸ ì™„ì„±
```

1. **WebSocket ì—°ê²°**
   - Frontend â†” Backend WebSocket ì„¤ì •
   - ì—°ê²° ìƒíƒœ ê´€ë¦¬

2. **ìŒì„± ìº¡ì²˜ & STT**
   - ë¸Œë¼ìš°ì € ë§ˆì´í¬ ìº¡ì²˜ (Web Audio API)
   - OpenAI Whisper API ì—°ë™
   - ì‹¤ì‹œê°„ ìë§‰ í‘œì‹œ
   - AI ìë™ í™”ì ë¶„ë¦¬ (ì°¸ì„ì ëª©ë¡ ê¸°ë°˜)

3. **Moderator Agent êµ¬í˜„**
   - OpenAI Agents SDK ì„¤ì •
   - ë°œí™” ë©ˆì¶¤ ê°ì§€ (1-2ì´ˆ ì¹¨ë¬µ)
   - ì£¼ì œ ì´íƒˆ ê°ì§€ ë¡œì§
   - ì›ì¹™ ìœ„ë°˜ ê°ì§€ (LLM íŒë‹¨)
   - ê°œì… ë©”ì‹œì§€ ìƒì„±

4. **ê°œì… UI**
   - ê²½ê³ ìŒ ì¬ìƒ (ì‹œì²­ê° ì£¼ì˜ í™˜ê¸°)
   - Toast ì»´í¬ë„ŒíŠ¸ í‘œì‹œ
   - ê°œì… íƒ€ì´ë°: ë°œí™”ìê°€ ë§ì„ ë©ˆì·„ì„ ë•Œ

### Phase 3: ë¶€ê°€ ê¸°ëŠ¥ (2ì‹œê°„)
```
Priority: HIGH
ëª©í‘œ: ë°ëª¨ ì™„ì„±ë„ í–¥ìƒ
```

1. **íšŒì˜ ì›ì¹™ ê´€ë¦¬ í˜ì´ì§€**
   - Agile ì›ì¹™ í…œí”Œë¦¿
   - AWS Leadership Principles í…œí”Œë¦¿
   - ì»¤ìŠ¤í…€ ì›ì¹™ ì¶”ê°€/ìˆ˜ì •/ì‚­ì œ
   - íšŒì˜ë³„ ì›ì¹™ ì„ íƒ

2. **ë°œì–¸ í†µê³„**
   - AI ìë™ í™”ì ë¶„ë¦¬ (GPT-4o í™œìš©)
   - ë°œì–¸ ë¹„ìœ¨ ê³„ì‚° & ì‹œê°í™”
   - ë¶ˆê· í˜• ê°ì§€ ì‹œ Agent ê°œì… íŠ¸ë¦¬ê±°

3. **íšŒì˜ ì¢…ë£Œ ì²˜ë¦¬**
   - ë¦¬ìº¡ ìƒì„±
   - Action Item ìë™ ì¶”ì¶œ
   - Markdown íŒŒì¼ ì €ì¥

### Phase 4: ë§ˆë¬´ë¦¬ (1ì‹œê°„)
```
Priority: MEDIUM
ëª©í‘œ: ë°ëª¨ ì¤€ë¹„
```

1. **ë°ëª¨ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ**
   - ë¯¸ë¦¬ ì¤€ë¹„ëœ ìŠ¤í¬ë¦½íŠ¸ ì¬ìƒ
   - ìˆ˜ë™ íŠ¸ë¦¬ê±° ë²„íŠ¼

2. **UI í´ë¦¬ì‹±**
   - ì• ë‹ˆë©”ì´ì…˜
   - ì—ëŸ¬ ì²˜ë¦¬ UI

---

## 2. ê¸°ìˆ  ìŠ¤íƒ ìƒì„¸

### Frontend
```json
{
  "framework": "Next.js 14",
  "language": "TypeScript",
  "styling": "Tailwind CSS",
  "components": "shadcn/ui",
  "state": "Zustand (ê¶Œì¥) ë˜ëŠ” React Context",
  "audio": "Web Audio API + MediaRecorder",
  "websocket": "native WebSocket",
  "alertSound": "HTML5 Audio API"
}
```

### Backend
```json
{
  "framework": "Python 3.12 + OpenAI Agents SDK",
  "webServer": "FastAPI (í•„ìš”ì‹œ)",
  "websocket": "websockets / fastapi.WebSocket",
  "ai": "OpenAI Agents SDK",
  "async": "asyncio",
  "storage": "ë¡œì»¬ íŒŒì¼ì‹œìŠ¤í…œ (.md)"
}
```

### OpenAI APIs
```json
{
  "stt": "OpenAI Realtime API (WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ STT)",
  "llm": "gpt-5.2 (Agent ì¶”ë¡ , ì›ì¹™ ìœ„ë°˜ ê°ì§€, í™”ì ë¶„ë¦¬)",
  "agents": "OpenAI Agents SDK (Multi-Agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)"
}
```

> **ì°¸ê³ **: TTSëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. ê°œì… ì‹œ **ì§§ê³  ë¶€ë“œëŸ¬ìš´ ì°¨ì„ë²¨ (1ì´ˆ ì´ë‚´)** + Toast ë©”ì‹œì§€ ë°©ì‹ ì‚¬ìš©.

---

## 3. í”„ë¡œì íŠ¸ êµ¬ì¡°

```
meetingmod/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”‚
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx              # Root layout
â”‚   â”‚   â”œâ”€â”€ page.tsx                # íšŒì˜ ì¤€ë¹„ í™”ë©´
â”‚   â”‚   â”œâ”€â”€ principles/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx            # íšŒì˜ ì›ì¹™ ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ meeting/
â”‚   â”‚   â”‚   â””â”€â”€ [id]/
â”‚   â”‚   â”‚       â””â”€â”€ page.tsx        # íšŒì˜ ì§„í–‰ í™”ë©´
â”‚   â”‚   â””â”€â”€ review/
â”‚   â”‚       â””â”€â”€ [id]/
â”‚   â”‚           â””â”€â”€ page.tsx        # íšŒì˜ ê²°ê³¼ í™”ë©´
â”‚   â”‚
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ui/                     # shadcn components
â”‚   â”‚   â”œâ”€â”€ meeting-prep-form.tsx   # íšŒì˜ ì¤€ë¹„ í¼
â”‚   â”‚   â”œâ”€â”€ markdown-editor.tsx     # ì•„ì  ë‹¤ ì—ë””í„°
â”‚   â”‚   â”œâ”€â”€ participant-list.tsx    # ì°¸ì„ì ëª©ë¡
â”‚   â”‚   â”œâ”€â”€ principle-editor.tsx    # ì›ì¹™ í¸ì§‘ê¸°
â”‚   â”‚   â”œâ”€â”€ meeting-room.tsx        # íšŒì˜ ì§„í–‰ ë©”ì¸
â”‚   â”‚   â”œâ”€â”€ transcript-view.tsx     # ì‹¤ì‹œê°„ ìë§‰ (í™”ìëª… í¬í•¨)
â”‚   â”‚   â”œâ”€â”€ speaker-stats.tsx       # ë°œì–¸ í†µê³„ ì‹œê°í™”
â”‚   â”‚   â”œâ”€â”€ intervention-toast.tsx  # Agent ê°œì… ì•Œë¦¼
â”‚   â”‚   â”œâ”€â”€ alert-sound.tsx         # ê²½ê³ ìŒ ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”œâ”€â”€ agenda-tracker.tsx      # ì•„ì  ë‹¤ ì§„í–‰ìƒí™©
â”‚   â”‚   â””â”€â”€ meeting-controls.tsx    # ì»¨íŠ¸ë¡¤ ë²„íŠ¼
â”‚   â”‚
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ websocket.ts            # WebSocket í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ audio-capture.ts        # ë§ˆì´í¬ ìº¡ì²˜
â”‚   â”‚   â”œâ”€â”€ sound-player.ts         # ê²½ê³ ìŒ ì¬ìƒ
â”‚   â”‚   â””â”€â”€ utils.ts                # ìœ í‹¸ë¦¬í‹°
â”‚   â”‚
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ use-websocket.ts        # WebSocket hook
â”‚   â”‚   â”œâ”€â”€ use-audio.ts            # ì˜¤ë””ì˜¤ hook
â”‚   â”‚   â””â”€â”€ use-meeting.ts          # íšŒì˜ ìƒíƒœ hook
â”‚   â”‚
â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â””â”€â”€ meeting-store.ts        # Zustand store
â”‚   â”‚
â”‚   â””â”€â”€ public/
â”‚       â””â”€â”€ sounds/
â”‚           â””â”€â”€ alert.wav           # ê²½ê³ ìŒ íŒŒì¼
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ main.py                     # ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (OpenAI Agents SDK)
â”‚   â”‚
â”‚   â”œâ”€â”€ server.py                   # FastAPI ì„œë²„ (í•„ìš”ì‹œ)
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ orchestrator.py         # Orchestrator Agent
â”‚   â”‚   â”œâ”€â”€ prep_agent.py           # Prep Agent
â”‚   â”‚   â”œâ”€â”€ moderator_agent.py      # Moderator Agent (í•µì‹¬)
â”‚   â”‚   â””â”€â”€ review_agent.py         # Review Agent
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ stt_service.py          # Whisper STT
â”‚   â”‚   â”œâ”€â”€ speaker_service.py      # AI í™”ì ë¶„ë¦¬
â”‚   â”‚   â””â”€â”€ storage_service.py      # Markdown íŒŒì¼ ì €ì¥
â”‚   â”‚
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ meeting.py              # íšŒì˜ ëª¨ë¸
â”‚       â””â”€â”€ intervention.py         # ê°œì… ëª¨ë¸
â”‚
â”œâ”€â”€ meetings/                        # íšŒì˜ ë°ì´í„° ì €ì¥
â”‚   â””â”€â”€ {YYYY-MM-DD-meeting-title}/
â”‚       â”œâ”€â”€ preparation.md          # íšŒì˜ ì¤€ë¹„ ìë£Œ
â”‚       â”œâ”€â”€ principles.md           # ì ìš©ëœ íšŒì˜ ì›ì¹™
â”‚       â”œâ”€â”€ transcript.md           # ë…¹ì·¨ë¡ (í™”ì í¬í•¨)
â”‚       â”œâ”€â”€ interventions.md        # Agent ê°œì… ê¸°ë¡
â”‚       â”œâ”€â”€ summary.md              # íšŒì˜ ìš”ì•½
â”‚       â””â”€â”€ action-items.md         # Action Items
â”‚
â””â”€â”€ principles/                      # íšŒì˜ ì›ì¹™ í…œí”Œë¦¿
    â”œâ”€â”€ agile.md
    â””â”€â”€ aws-leadership.md
```

---

## 4. í•µì‹¬ êµ¬í˜„ ê°€ì´ë“œ

### 4.1 WebSocket ì—°ê²°

**Frontend (lib/websocket.ts)**
```typescript
export class MeetingWebSocket {
  private ws: WebSocket | null = null;
  private meetingId: string;
  private onMessage: (message: any) => void;

  constructor(meetingId: string, onMessage: (message: any) => void) {
    this.meetingId = meetingId;
    this.onMessage = onMessage;
  }

  connect() {
    this.ws = new WebSocket(`ws://localhost:8000/ws/meetings/${this.meetingId}`);

    this.ws.onmessage = (event) => {
      const message = JSON.parse(event.data);
      this.onMessage(message);
    };

    this.ws.onerror = (error) => {
      console.error('WebSocket error:', error);
    };
  }

  sendAudio(audioData: Blob) {
    if (this.ws?.readyState === WebSocket.OPEN) {
      // Convert to base64 and send
      const reader = new FileReader();
      reader.onloadend = () => {
        this.ws?.send(JSON.stringify({
          type: 'audio',
          data: reader.result,
          timestamp: Date.now()
        }));
      };
      reader.readAsDataURL(audioData);
    }
  }

  disconnect() {
    this.ws?.close();
  }
}
```

**Backend (routers/websocket.py)**
```python
from fastapi import WebSocket, WebSocketDisconnect
from agents.moderator_agent import ModeratorAgent
from services.stt_service import STTService

class ConnectionManager:
    def __init__(self):
        self.active_connections: dict[str, WebSocket] = {}

    async def connect(self, meeting_id: str, websocket: WebSocket):
        await websocket.accept()
        self.active_connections[meeting_id] = websocket

    async def disconnect(self, meeting_id: str):
        if meeting_id in self.active_connections:
            del self.active_connections[meeting_id]

    async def send_message(self, meeting_id: str, message: dict):
        if meeting_id in self.active_connections:
            await self.active_connections[meeting_id].send_json(message)

manager = ConnectionManager()

@router.websocket("/ws/meetings/{meeting_id}")
async def websocket_endpoint(websocket: WebSocket, meeting_id: str):
    await manager.connect(meeting_id, websocket)
    stt_service = STTService()
    moderator = ModeratorAgent(meeting_id)

    try:
        while True:
            data = await websocket.receive_json()

            if data["type"] == "audio":
                # STT ì²˜ë¦¬
                transcript = await stt_service.transcribe(data["data"])

                # ìë§‰ ì „ì†¡
                await manager.send_message(meeting_id, {
                    "type": "transcript",
                    "data": transcript
                })

                # Agent ë¶„ì„ (ë²„í¼ì— ì¶•ì )
                intervention = await moderator.analyze(transcript)
                if intervention:
                    await manager.send_message(meeting_id, {
                        "type": "intervention",
                        "data": intervention
                    })

    except WebSocketDisconnect:
        await manager.disconnect(meeting_id)
```

### 4.2 Moderator Agent êµ¬í˜„ (ë°œí™” ë©ˆì¶¤ ê°ì§€ + ê²½ê³ ìŒ)

**agents/moderator_agent.py**
```python
from openai import OpenAI
import json
import time
import uuid
from datetime import datetime

class ModeratorAgent:
    def __init__(self, meeting_id: str):
        self.client = OpenAI()
        self.meeting_id = meeting_id
        self.transcript_buffer = []
        self.agenda = []
        self.principles = []
        self.participants = []
        self.speaker_stats = {}
        self.last_intervention_time = 0
        self.last_speech_time = 0
        self.silence_threshold = 1.5  # 1.5ì´ˆ ì¹¨ë¬µ ì‹œ ê°œì… íŒë‹¨

    def set_context(self, agenda: list, principles: list, participants: list):
        self.agenda = agenda
        self.principles = principles
        self.participants = participants

    async def on_speech_end(self, silence_duration: float) -> dict | None:
        """
        ë°œí™”ìê°€ ë§ì„ ë©ˆì·„ì„ ë•Œ í˜¸ì¶œë¨.
        silence_duration: ì¹¨ë¬µ ì§€ì† ì‹œê°„ (ì´ˆ)
        """
        if silence_duration < self.silence_threshold:
            return None

        # ìµœì†Œ ê°œì… ê°„ê²© ì²´í¬ (20ì´ˆ)
        current_time = time.time()
        if current_time - self.last_intervention_time < 20:
            return None

        # ê°œì… í•„ìš”ì„± íŒë‹¨
        intervention = await self._check_intervention()
        if intervention:
            self.last_intervention_time = current_time
            # ê°œì… ì‹œ ê²½ê³ ìŒ ì¬ìƒ í”Œë˜ê·¸ í¬í•¨
            intervention["playAlertSound"] = True
            return intervention

        return None

    async def add_transcript(self, transcript_entry: dict):
        """ìƒˆ ë°œí™” ì¶”ê°€"""
        self.transcript_buffer.append(transcript_entry)
        self._update_speaker_stats(transcript_entry)
        self.last_speech_time = time.time()

    async def _check_intervention(self) -> dict | None:
        """GPT-4oë¡œ ê°œì… í•„ìš”ì„± ë¶„ì„"""
        if len(self.transcript_buffer) < 3:
            return None

        recent_transcript = self.transcript_buffer[-10:]  # ìµœê·¼ 10ê°œ ë°œí™”

        system_prompt = f"""You are an AI meeting moderator.
Your role is to monitor the meeting and intervene actively when the speaker stops talking.

Meeting agenda: {json.dumps(self.agenda, ensure_ascii=False)}
Meeting principles: {json.dumps(self.principles, ensure_ascii=False)}
Participants: {json.dumps(self.participants, ensure_ascii=False)}
Speaker statistics: {json.dumps(self.speaker_stats, ensure_ascii=False)}

Analyze the recent conversation and determine if intervention is needed.
BE PROACTIVE - intervene when you detect issues.

Intervention types:
- TOPIC_DRIFT: When discussion goes off-topic (respond with parking lot suggestion)
- PRINCIPLE_VIOLATION: When someone violates meeting principles (e.g., top-down decision, not respecting others)
- PARTICIPATION_IMBALANCE: When participation is uneven (one person dominates, someone hasn't spoken)
- DECISION_STYLE: When top-down decision making is detected (if horizontal decision is a principle)

INTERVENTION TONE: Be DIRECT and COURAGEOUS. Examples:
- BAD: "ì¬ë¯¸ìˆëŠ” ì´ì•¼ê¸°ë„¤ìš”! ë‹¤ë§Œ ì‹œê°„ ê´€ê³„ìƒ..."
- GOOD: "ì ê¹ìš”, ì•„ì  ë‹¤ì—ì„œ ë²—ì–´ë‚¬ì–´ìš”. ëŒì•„ê°ˆê²Œìš”."
- BAD: "ì˜ê²¬ì´ ìˆìœ¼ì‹¤ê¹Œìš”?"
- GOOD: "ì ê¹ìš”! ì•„ì§ ë°œì–¸ ì•ˆ í•˜ì…¨ì–´ìš”. ì–´ë–»ê²Œ ë³´ì„¸ìš”?"
- BAD: "ë‹¤ë¥¸ ë¶„ë“¤ ì˜ê²¬ì€ ì–´ë– ì‹ ê°€ìš”?"
- GOOD: "ë©ˆì¶°ì£¼ì„¸ìš”! ì›ì¹™ ìœ„ë°˜ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶„ë“¤ ë™ì˜í•˜ì‹œë‚˜ìš”?"

Respond in JSON format:
{{
  "needs_intervention": true/false,
  "intervention_type": "TOPIC_DRIFT" | "PRINCIPLE_VIOLATION" | "PARTICIPATION_IMBALANCE" | "DECISION_STYLE" | null,
  "message": "ê°œì… ë©”ì‹œì§€ (í•œêµ­ì–´, ì§ì ‘ì ì´ê³  ìš©ê¸°ìˆëŠ” í†¤)",
  "violated_principle": "ìœ„ë°˜ëœ ì›ì¹™ëª…" | null,
  "parking_lot_item": "ì£¼ì œ ì´íƒˆ ì‹œ ì¶”ê°€í•  í•­ëª©" | null,
  "suggested_speaker": "ë°œì–¸ ê¶Œìœ í•  ì°¸ì„ì ì´ë¦„" | null
}}
"""

        response = self.client.chat.completions.create(
            model="gpt-5.2",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(recent_transcript, ensure_ascii=False)}
            ],
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)

        if result.get("needs_intervention"):
            return {
                "id": f"int_{uuid.uuid4().hex[:8]}",
                "timestamp": datetime.utcnow().isoformat(),
                "type": result["intervention_type"],
                "message": result["message"],
                "violatedPrinciple": result.get("violated_principle"),
                "parkingLotItem": result.get("parking_lot_item"),
                "suggestedSpeaker": result.get("suggested_speaker"),
                "triggerContext": recent_transcript[-1].get("text", "") if recent_transcript else ""
            }

        return None

    def _update_speaker_stats(self, entry: dict):
        speaker = entry.get("speaker", "Unknown")
        text_length = len(entry.get("text", ""))
        duration = entry.get("duration", 0)

        if speaker not in self.speaker_stats:
            self.speaker_stats[speaker] = {
                "count": 0,
                "chars": 0,
                "duration": 0
            }

        self.speaker_stats[speaker]["count"] += 1
        self.speaker_stats[speaker]["chars"] += text_length
        self.speaker_stats[speaker]["duration"] += duration
```

### 4.3 Audio Capture (Frontend) + ì¹¨ë¬µ ê°ì§€

**lib/audio-capture.ts**
```typescript
export class AudioCapture {
  private mediaRecorder: MediaRecorder | null = null;
  private stream: MediaStream | null = null;
  private audioContext: AudioContext | null = null;
  private analyser: AnalyserNode | null = null;
  private onDataAvailable: (data: Blob) => void;
  private onSilenceDetected: (silenceDuration: number) => void;
  private silenceStart: number | null = null;
  private silenceThreshold = 0.01; // ë³¼ë¥¨ ì„ê³„ê°’

  constructor(
    onDataAvailable: (data: Blob) => void,
    onSilenceDetected: (silenceDuration: number) => void
  ) {
    this.onDataAvailable = onDataAvailable;
    this.onSilenceDetected = onSilenceDetected;
  }

  async start() {
    try {
      this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // ì¹¨ë¬µ ê°ì§€ë¥¼ ìœ„í•œ AudioContext ì„¤ì •
      this.audioContext = new AudioContext();
      const source = this.audioContext.createMediaStreamSource(this.stream);
      this.analyser = this.audioContext.createAnalyser();
      this.analyser.fftSize = 256;
      source.connect(this.analyser);

      // ì¹¨ë¬µ ê°ì§€ ì‹œì‘
      this.detectSilence();

      this.mediaRecorder = new MediaRecorder(this.stream, {
        mimeType: 'audio/webm;codecs=opus'
      });

      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.onDataAvailable(event.data);
        }
      };

      // 3ì´ˆë§ˆë‹¤ ì²­í¬ ì „ì†¡
      this.mediaRecorder.start(3000);
    } catch (error) {
      console.error('Failed to start audio capture:', error);
      throw error;
    }
  }

  private detectSilence() {
    if (!this.analyser) return;

    const bufferLength = this.analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    const checkVolume = () => {
      if (!this.analyser) return;

      this.analyser.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((a, b) => a + b) / bufferLength / 255;

      if (average < this.silenceThreshold) {
        // ì¹¨ë¬µ ì‹œì‘ ë˜ëŠ” ì§€ì†
        if (this.silenceStart === null) {
          this.silenceStart = Date.now();
        } else {
          const silenceDuration = (Date.now() - this.silenceStart) / 1000;
          // 1.5ì´ˆ ì´ìƒ ì¹¨ë¬µ ì‹œ ì½œë°± í˜¸ì¶œ
          if (silenceDuration >= 1.5) {
            this.onSilenceDetected(silenceDuration);
            this.silenceStart = null; // ë¦¬ì…‹
          }
        }
      } else {
        // ì†Œë¦¬ ê°ì§€ë¨ - ì¹¨ë¬µ ë¦¬ì…‹
        this.silenceStart = null;
      }

      requestAnimationFrame(checkVolume);
    };

    checkVolume();
  }

  stop() {
    this.mediaRecorder?.stop();
    this.stream?.getTracks().forEach(track => track.stop());
    this.audioContext?.close();
  }

  pause() {
    this.mediaRecorder?.pause();
  }

  resume() {
    this.mediaRecorder?.resume();
  }
}
```

### 4.4 ê²½ê³ ìŒ ì¬ìƒ

**ê²½ê³ ìŒ ìŠ¤í™**
- **íƒ€ì…**: ì§§ê³  ë¶€ë“œëŸ¬ìš´ ì°¨ì„ë²¨
- **ê¸¸ì´**: 1ì´ˆ ì´ë‚´
- **ë³¼ë¥¨**: 0.7 (70%)
- **íŒŒì¼**: `/public/sounds/alert-chime.wav`
- **ê¶Œì¥ ì†ŒìŠ¤**:
  - https://freesound.org (ê²€ìƒ‰: "soft chime notification")
  - https://mixkit.co/free-sound-effects/notification/
  - https://notificationsounds.com/notification-sounds

**lib/sound-player.ts**
```typescript
export class SoundPlayer {
  private audio: HTMLAudioElement;
  private isReady: boolean = false;

  constructor() {
    // ì§§ê³  ë¶€ë“œëŸ¬ìš´ ì°¨ì„ë²¨ (1ì´ˆ ì´ë‚´)
    this.audio = new Audio('/sounds/alert-chime.wav');
    this.audio.volume = 0.7;

    // í”„ë¦¬ë¡œë“œí•˜ì—¬ ì§€ì—° ìµœì†Œí™”
    this.audio.addEventListener('canplaythrough', () => {
      this.isReady = true;
    });
    this.audio.load();
  }

  playAlert() {
    if (!this.isReady) {
      console.warn('Alert sound not ready yet');
      return;
    }
    this.audio.currentTime = 0;
    this.audio.play().catch(err => {
      console.warn('Alert sound failed to play:', err);
    });
  }

  // ë³¼ë¥¨ ì¡°ì ˆ (0.0 ~ 1.0)
  setVolume(volume: number) {
    this.audio.volume = Math.max(0, Math.min(1, volume));
  }
}
```

**components/intervention-toast.tsx**
```typescript
import { useEffect } from 'react';
import { Toast, ToastTitle, ToastDescription } from '@/components/ui/toast';
import { SoundPlayer } from '@/lib/sound-player';

interface InterventionToastProps {
  intervention: {
    type: string;
    message: string;
    playAlertSound?: boolean;
  } | null;
  onClose: () => void;
}

const soundPlayer = new SoundPlayer();

export function InterventionToast({ intervention, onClose }: InterventionToastProps) {
  useEffect(() => {
    if (intervention?.playAlertSound) {
      soundPlayer.playAlert();
    }
  }, [intervention]);

  if (!intervention) return null;

  const typeLabels = {
    TOPIC_DRIFT: 'ğŸ¯ ì£¼ì œ ì´íƒˆ',
    PRINCIPLE_VIOLATION: 'âš ï¸ ì›ì¹™ ìœ„ë°˜',
    PARTICIPATION_IMBALANCE: 'âš–ï¸ ë°œì–¸ ë¶ˆê· í˜•',
    DECISION_STYLE: 'ğŸ¤ ì˜ì‚¬ê²°ì • ë°©ì‹'
  };

  return (
    <Toast open={!!intervention} onOpenChange={() => onClose()}>
      <ToastTitle>{typeLabels[intervention.type] || 'ğŸ¤– AI ê°œì…'}</ToastTitle>
      <ToastDescription>{intervention.message}</ToastDescription>
    </Toast>
  );
}
```

### 4.5 Realtime API STT Service (Backend)

**services/realtime_stt_service.py**
```python
import asyncio
import websockets
import json
import uuid
from datetime import datetime
from typing import Callable, Optional

REALTIME_API_URL = "wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17"

class RealtimeSTTService:
    """OpenAI Realtime API ê¸°ë°˜ ì‹¤ì‹œê°„ STT ì„œë¹„ìŠ¤"""

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.ws: Optional[websockets.WebSocketClientProtocol] = None
        self.on_transcript: Optional[Callable] = None
        self.on_speech_end: Optional[Callable] = None

    async def connect(self, on_transcript: Callable, on_speech_end: Callable):
        """Realtime API ì—°ê²° ë° ì„¸ì…˜ ì„¤ì •"""
        self.on_transcript = on_transcript
        self.on_speech_end = on_speech_end

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "OpenAI-Beta": "realtime=v1"
        }

        self.ws = await websockets.connect(
            REALTIME_API_URL,
            extra_headers=headers
        )

        # ì„¸ì…˜ ì„¤ì •: ì‹¤ì‹œê°„ ì „ì‚¬ + 1.5ì´ˆ ì¹¨ë¬µ ê°ì§€
        await self.ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "input_audio_format": "pcm16",
                "input_audio_transcription": {
                    "model": "whisper-1"
                },
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 1500  # 1.5ì´ˆ ì¹¨ë¬µ ì‹œ ë°œí™” ì¢…ë£Œ ê°ì§€
                }
            }
        }))

        # ë©”ì‹œì§€ ìˆ˜ì‹  ë£¨í”„ ì‹œì‘
        asyncio.create_task(self._receive_loop())

    async def _receive_loop(self):
        """Realtime API ë©”ì‹œì§€ ìˆ˜ì‹  ë£¨í”„"""
        try:
            async for message in self.ws:
                data = json.loads(message)
                event_type = data.get("type", "")

                # ì‹¤ì‹œê°„ ì „ì‚¬ ì™„ë£Œ
                if event_type == "conversation.item.input_audio_transcription.completed":
                    transcript = data.get("transcript", "")
                    if transcript and self.on_transcript:
                        await self.on_transcript({
                            "id": f"tr_{uuid.uuid4().hex[:8]}",
                            "text": transcript,
                            "timestamp": datetime.utcnow().isoformat(),
                            "isFinal": True
                        })

                # ë°œí™” ì¢…ë£Œ ê°ì§€ (ì¹¨ë¬µ 1.5ì´ˆ)
                elif event_type == "input_audio_buffer.speech_stopped":
                    if self.on_speech_end:
                        await self.on_speech_end()

        except websockets.exceptions.ConnectionClosed:
            print("Realtime API connection closed")

    async def send_audio(self, audio_base64: str):
        """ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡"""
        if self.ws:
            await self.ws.send(json.dumps({
                "type": "input_audio_buffer.append",
                "audio": audio_base64
            }))

    async def disconnect(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.ws:
            await self.ws.close()
```

**services/speaker_service.py**
```python
from openai import OpenAI
import json

class SpeakerService:
    """GPT-5.2 ê¸°ë°˜ í™”ì ë¶„ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.client = OpenAI()
        self.participants = []
        self.recent_speakers = []  # ìµœê·¼ í™”ì ê¸°ë¡

    def set_participants(self, participants: list):
        """ì°¸ì„ì ëª©ë¡ ì„¤ì •"""
        self.participants = participants

    async def identify_speaker(self, text: str, context: list[dict]) -> dict:
        """
        ë°œí™” í…ìŠ¤íŠ¸ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™”ì ì¶”ë¡  (GPT-5.2 í™œìš©).
        context: ìµœê·¼ ë°œí™” ê¸°ë¡ (speaker, text)
        """
        if not self.participants:
            return {"speaker": "Unknown", "confidence": 0.0}

        participant_info = json.dumps(
            [{"name": p["name"], "role": p["role"]} for p in self.participants],
            ensure_ascii=False
        )

        recent_context = json.dumps(context[-5:], ensure_ascii=False) if context else "[]"

        prompt = f"""You are a speaker identification AI using GPT-5.2's advanced context understanding.
Based on the participant list, recent conversation context, and the new utterance,
identify who is most likely speaking.

Participants:
{participant_info}

Recent conversation (for context):
{recent_context}

New utterance to identify:
"{text}"

Consider:
1. Speaking patterns and vocabulary
2. Role-appropriate topics (e.g., PM talks about deadlines, developer talks about code)
3. Conversation flow (who would logically respond)
4. Korean honorifics and speech patterns
5. Emotional tone and formality level

Respond in JSON format:
{{
  "speaker": "í™”ì ì´ë¦„ (from participant list)",
  "confidence": 0.0-1.0,
  "reasoning": "ê°„ë‹¨í•œ ì¶”ë¡  ì´ìœ "
}}
"""

        response = self.client.chat.completions.create(
            model="gpt-5.2",
            messages=[
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)

        # í™”ì ê¸°ë¡ ì—…ë°ì´íŠ¸
        self.recent_speakers.append(result["speaker"])
        if len(self.recent_speakers) > 10:
            self.recent_speakers.pop(0)

        return {
            "speaker": result["speaker"],
            "confidence": result["confidence"]
        }
```

### 4.6 Markdown íŒŒì¼ ì €ì¥ ì„œë¹„ìŠ¤

**services/storage_service.py**
```python
import os
from datetime import datetime
from pathlib import Path

class StorageService:
    """Markdown íŒŒì¼ ì €ì¥ ì„œë¹„ìŠ¤"""

    def __init__(self, base_path: str = "meetings"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)

    def get_meeting_dir(self, meeting_id: str) -> Path:
        """íšŒì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜ (ì—†ìœ¼ë©´ ìƒì„±)"""
        meeting_dir = self.base_path / meeting_id
        meeting_dir.mkdir(exist_ok=True)
        return meeting_dir

    async def save_transcript(self, meeting_id: str, entries: list[dict], title: str):
        """ë…¹ì·¨ë¡ ì €ì¥"""
        meeting_dir = self.get_meeting_dir(meeting_id)
        filepath = meeting_dir / "transcript.md"

        content = f"""# íšŒì˜ ë…¹ì·¨ë¡

íšŒì˜: {title}
ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M')}

---

"""
        for entry in entries:
            timestamp = entry.get("timestamp", "")[:19].replace("T", " ")
            speaker = entry.get("speaker", "Unknown")
            text = entry.get("text", "")
            content += f"[{timestamp}] **{speaker}**: {text}\n\n"

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)

    async def save_interventions(self, meeting_id: str, interventions: list[dict], title: str):
        """ê°œì… ê¸°ë¡ ì €ì¥"""
        meeting_dir = self.get_meeting_dir(meeting_id)
        filepath = meeting_dir / "interventions.md"

        content = f"""# Agent ê°œì… ê¸°ë¡

íšŒì˜: {title}
ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d')}

---

"""
        for idx, inv in enumerate(interventions, 1):
            content += f"""## ê°œì… #{idx}

- **ì‹œê°„**: {inv.get("timestamp", "")[:19].replace("T", " ")}
- **ìœ í˜•**: {inv.get("type", "")}
- **ë©”ì‹œì§€**: {inv.get("message", "")}
"""
            if inv.get("violatedPrinciple"):
                content += f"- **ìœ„ë°˜ ì›ì¹™**: {inv['violatedPrinciple']}\n"
            if inv.get("parkingLotItem"):
                content += f"- **Parking Lot**: {inv['parkingLotItem']}\n"
            content += "\n"

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)

    async def save_action_items(self, meeting_id: str, items: list[dict], title: str):
        """Action Items ì €ì¥"""
        meeting_dir = self.get_meeting_dir(meeting_id)
        filepath = meeting_dir / "action-items.md"

        content = f"""# Action Items

íšŒì˜: {title}
ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d')}

---

## í• ë‹¹ëœ ì—…ë¬´

"""
        for idx, item in enumerate(items, 1):
            content += f"""### {idx}. {item.get("description", "")}
- **ë‹´ë‹¹**: {item.get("assignee", "ë¯¸ì •")}
- **ê¸°í•œ**: {item.get("dueDate", "ë¯¸ì •")}
- **ë§¥ë½**: {item.get("context", "")}

"""

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)

    async def save_summary(self, meeting_id: str, summary: str, title: str):
        """íšŒì˜ ìš”ì•½ ì €ì¥"""
        meeting_dir = self.get_meeting_dir(meeting_id)
        filepath = meeting_dir / "summary.md"

        content = f"""# íšŒì˜ ìš”ì•½

íšŒì˜: {title}
ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d')}

---

{summary}
"""

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
```

---

## 5. í™˜ê²½ ì„¤ì •

### Frontend (.env.local)
```env
NEXT_PUBLIC_WS_URL=ws://localhost:8000
NEXT_PUBLIC_API_URL=http://localhost:8000/api/v1
```

### Backend (.env)
```env
OPENAI_API_KEY=sk-xxx
CORS_ORIGINS=http://localhost:3000
DEBUG=true
```

### í•„ìˆ˜ íŒ¨í‚¤ì§€

**Frontend (package.json dependencies)**
```json
{
  "next": "^14.0.0",
  "react": "^18.2.0",
  "typescript": "^5.0.0",
  "tailwindcss": "^3.4.0",
  "@radix-ui/react-*": "latest",
  "zustand": "^4.5.0",
  "lucide-react": "latest",
  "class-variance-authority": "latest",
  "clsx": "latest"
}
```

**Backend (requirements.txt)**
```
openai-agents>=0.1.0
fastapi>=0.109.0
uvicorn[standard]>=0.27.0
websockets>=12.0
openai>=1.10.0
python-dotenv>=1.0.0
pydantic>=2.0.0
```

> **ì°¸ê³ **: Python 3.12 ì‚¬ìš©. `openai-agents`ëŠ” OpenAI Agents SDK.

---

## 6. ì‹¤í–‰ ëª…ë ¹ì–´

### Frontend
```bash
cd frontend
npm install
npm run dev
# http://localhost:3000
```

### Backend
```bash
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
python main.py
# ë˜ëŠ” FastAPI ì‚¬ìš© ì‹œ:
# uvicorn server:app --reload --port 8000
# http://localhost:8000
```

> **ì°¸ê³ **: Python 3.12 í•„ìˆ˜. OpenAI Agents SDK ì‚¬ìš©.

---

## 7. ë°ëª¨ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ

í•´ì»¤í†¤ ë°œí‘œ ì‹œ ì•ˆì •ì ì¸ ë°ëª¨ë¥¼ ìœ„í•´ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ êµ¬í˜„:

**frontend/lib/demo-simulator.ts**
```typescript
export const demoScript = [
  { speaker: "ê¹€ì² ìˆ˜", text: "ì§€ë‚œ ìŠ¤í”„ë¦°íŠ¸ì—ì„œ 8ê°œ íƒœìŠ¤í¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.", delay: 0 },
  { speaker: "ì´ë¯¼ìˆ˜", text: "ë„¤, ì„±ê³¼ê°€ ì¢‹ì•˜ì–´ìš”.", delay: 3000 },
  { speaker: "ì´ë¯¼ìˆ˜", text: "ê·¸ëŸ°ë° ì ì‹¬ ë­ ë¨¹ì„ê¹Œìš”?", delay: 6000, triggerIntervention: "TOPIC_DRIFT" },
  { speaker: "ê¹€ì² ìˆ˜", text: "ì•„, ë„¤. ë‹¤ìŒ ìŠ¤í”„ë¦°íŠ¸ ê³„íšì„ ë³´ë©´...", delay: 12000 },
  { speaker: "ê¹€ì² ìˆ˜", text: "ì´ë²ˆ ìŠ¤í”„ë¦°íŠ¸ëŠ” API ìµœì í™”ì— ì§‘ì¤‘í•©ì‹œë‹¤.", delay: 15000 },
  { speaker: "ê¹€ì² ìˆ˜", text: "ë°•ì˜í¬ ì”¨, ë™ì˜í•˜ì‹œì£ ?", delay: 18000, triggerIntervention: "DECISION_STYLE" },
  // ... ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤
];

export class DemoSimulator {
  private scriptIndex = 0;
  private onTranscript: (entry: any) => void;
  private onIntervention: (intervention: any) => void;
  private soundPlayer: { playAlert: () => void };

  constructor(
    onTranscript: (entry: any) => void,
    onIntervention: (intervention: any) => void,
    soundPlayer: { playAlert: () => void }
  ) {
    this.onTranscript = onTranscript;
    this.onIntervention = onIntervention;
    this.soundPlayer = soundPlayer;
  }

  start() {
    this.playNext();
  }

  private playNext() {
    if (this.scriptIndex >= demoScript.length) return;

    const entry = demoScript[this.scriptIndex];

    setTimeout(() => {
      this.onTranscript({
        speaker: entry.speaker,
        text: entry.text,
        timestamp: new Date().toISOString()
      });

      if (entry.triggerIntervention) {
        // ë°œí™” ë©ˆì¶¤ í›„ 1.5ì´ˆ ë’¤ ê°œì… (ì¹¨ë¬µ ê°ì§€ ì‹œë®¬ë ˆì´ì…˜)
        setTimeout(() => {
          // ê²½ê³ ìŒ ì¬ìƒ
          this.soundPlayer.playAlert();
          // Toast í‘œì‹œ
          this.onIntervention(this.getIntervention(entry.triggerIntervention));
        }, 1500);
      }

      this.scriptIndex++;
      this.playNext();
    }, entry.delay);
  }

  private getIntervention(type: string) {
    const interventions: Record<string, any> = {
      TOPIC_DRIFT: {
        id: "int_demo_001",
        type: "TOPIC_DRIFT",
        message: "ì¬ë¯¸ìˆëŠ” ì´ì•¼ê¸°ë„¤ìš”! ë‹¤ë§Œ ì‹œê°„ ê´€ê³„ìƒ, í˜„ì¬ ë…¼ì˜ ì¤‘ì¸ 'ìŠ¤í”„ë¦°íŠ¸ ê³„íš'ì„ ë¨¼ì € ë§ˆë¬´ë¦¬í•˜ë©´ ì–´ë–¨ê¹Œìš”?",
        parkingLotItem: "ì ì‹¬ ë©”ë‰´",
        playAlertSound: true
      },
      DECISION_STYLE: {
        id: "int_demo_002",
        type: "DECISION_STYLE",
        message: "ì ê¹ìš”! ì¤‘ìš”í•œ ê²°ì • ì „ì—, ì•„ì§ ì˜ê²¬ì„ ë§ì”€í•˜ì§€ ì•Šìœ¼ì‹  ë°•ì˜í¬ ë‹˜ì˜ ìƒê°ë„ ë“¤ì–´ë³´ë©´ ì–´ë–¨ê¹Œìš”?",
        violatedPrinciple: "ìˆ˜í‰ì  ì˜ì‚¬ê²°ì •",
        suggestedSpeaker: "ë°•ì˜í¬",
        playAlertSound: true
      },
      PARTICIPATION_IMBALANCE: {
        id: "int_demo_003",
        type: "PARTICIPATION_IMBALANCE",
        message: "ì§€ê¸ˆê¹Œì§€ ê¹€ì² ìˆ˜ ë‹˜ì´ ë°œì–¸ì˜ 70%ë¥¼ ì°¨ì§€í•˜ê³  ê³„ì„¸ìš”. ë‹¤ë¥¸ ë¶„ë“¤ì˜ ì˜ê²¬ë„ ë“¤ì–´ë³¼ê¹Œìš”?",
        playAlertSound: true
      },
      PRINCIPLE_VIOLATION: {
        id: "int_demo_004",
        type: "PRINCIPLE_VIOLATION",
        message: "ë§ì”€ ì¤‘ì— ì£„ì†¡í•©ë‹ˆë‹¤. 'Disagree and Commit' ì›ì¹™ì— ë”°ë¼, ì´ê²¬ì´ ìˆìœ¼ì‹œë©´ ì§€ê¸ˆ ë§ì”€í•´ ì£¼ì„¸ìš”.",
        violatedPrinciple: "Disagree and Commit",
        playAlertSound: true
      }
    };
    return interventions[type] || interventions.TOPIC_DRIFT;
  }
}
```

---

## 8. í•µì‹¬ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Next.js 14 í”„ë¡œì íŠ¸ ìƒì„± (`npx create-next-app@latest`)
- [ ] shadcn/ui ì„¤ì¹˜ (`npx shadcn-ui@latest init`)
- [ ] Python 3.12 í™˜ê²½ êµ¬ì„±
- [ ] OpenAI Agents SDK ì„¤ì¹˜
- [ ] ê¸°ë³¸ ë ˆì´ì•„ì›ƒ ë° ë¼ìš°íŒ… êµ¬ì¡°

### Phase 2 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] WebSocket ì—°ê²° (Frontend â†” Backend)
- [ ] ë§ˆì´í¬ ì˜¤ë””ì˜¤ ìº¡ì²˜ (Web Audio API)
- [ ] ì¹¨ë¬µ ê°ì§€ ë¡œì§ (1.5ì´ˆ ì„ê³„ê°’)
- [ ] Whisper STT ì—°ë™ (í•œêµ­ì–´)
- [ ] AI í™”ì ë¶„ë¦¬ ì„œë¹„ìŠ¤
- [ ] Moderator Agent êµ¬í˜„ (ë°œí™” ë©ˆì¶¤ ì‹œ ê°œì…)
- [ ] ê²½ê³ ìŒ ì¬ìƒ ì»´í¬ë„ŒíŠ¸
- [ ] Toast ê°œì… ë©”ì‹œì§€ UI

### Phase 3 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] íšŒì˜ ì›ì¹™ ê´€ë¦¬ í˜ì´ì§€ (`/principles`)
- [ ] Agile/AWS LP í…œí”Œë¦¿
- [ ] ë°œì–¸ í†µê³„ ì‹œê°í™” (Progress Bar)
- [ ] íšŒì˜ ì¢…ë£Œ â†’ ë¦¬ìº¡ ìƒì„±
- [ ] Action Item ì¶”ì¶œ (LLM)
- [ ] Markdown íŒŒì¼ ì €ì¥

### Phase 4 ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë°ëª¨ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
- [ ] ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„
- [ ] UI í´ë¦¬ì‹± ë° ì—ëŸ¬ ì²˜ë¦¬
- [ ] ë°œí‘œ ë¦¬í—ˆì„¤

---

## 9. ê°œì… íƒ€ì´ë° í”Œë¡œìš°ì°¨íŠ¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ì‹¤ì‹œê°„ ìŒì„± ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ë§ˆì´í¬ ì˜¤ë””ì˜¤ ìº¡ì²˜ â”‚
â”‚ (Web Audio API) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WebSocket ì „ì†¡   â”‚â”€â”€â”€â”€â–¶â”‚ Whisper STT     â”‚
â”‚ (3ì´ˆ ì²­í¬)       â”‚     â”‚ (í•œêµ­ì–´ ë³€í™˜)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ AI í™”ì ë¶„ë¦¬     â”‚
                        â”‚ (GPT-4o)        â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                               â”‚
         â–¼                                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ì‹¤ì‹œê°„ ìë§‰ í‘œì‹œ  â”‚                           â”‚ ì¹¨ë¬µ ê°ì§€        â”‚
â”‚ (í™”ìëª… + ë°œí™”)  â”‚                           â”‚ (1.5ì´ˆ ì„ê³„ê°’)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                              ì¹¨ë¬µ 1.5ì´ˆ ì´ìƒ?
                                                       â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚                                     â”‚
                                   No                                    Yes
                                    â”‚                                     â”‚
                                    â–¼                                     â–¼
                           ë‹¤ìŒ ë°œí™” ëŒ€ê¸°                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                â”‚ Moderator Agent â”‚
                                                                â”‚ ê°œì… ë¶„ì„       â”‚
                                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                         â”‚
                                                              ê°œì… í•„ìš”?
                                                                         â”‚
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚                                         â”‚
                                                   No                                        Yes
                                                    â”‚                                         â”‚
                                                    â–¼                                         â–¼
                                           ë‹¤ìŒ ë°œí™” ëŒ€ê¸°                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                                    â”‚ ê²½ê³ ìŒ ì¬ìƒ ğŸ””   â”‚
                                                                                    â”‚ + Toast í‘œì‹œ    â”‚
                                                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
